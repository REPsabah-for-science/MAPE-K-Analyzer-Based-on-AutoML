# -*- coding: utf-8 -*-
"""Computing-Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdAsT8NbcD_VsEt4lCcoojsEKWuhX4sS
"""

from google.colab import drive
drive.mount('/content/drive')

!python -m pip install --upgrade pip
!pip install --upgrade tensorflow
!pip install --upgrade scikit-learn
!pip install autogluon.tabular[all]

"""                                   ----------------------------------  Dataset Preprocessing ----------------------------------"""

# CREATION OF ONE DATASET WITH BINARY CLASSIFICATION AND FEATURE SELECTION
# This code identifies the indices of the selected features.
from sklearn.ensemble import ExtraTreesClassifier
import json
import numpy as np
import time

# Number of files to process
c = 300

# Load data function
def loadData():
    features = []
    packetLoss = []
    latency = []

    # Load data from JSON files
    for i in range(1, c + 1):
        data_path = f'/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/raw/dataset_with_all_features{i}.json'
        with open(data_path) as file:
            data = json.load(file)
            features.extend(data['features'])
            packetLoss.extend(data['packetloss'])
            latency.extend(data['latency'])

    # Create binary classification target based on packet loss and latency
    classificationTargets = [(1 if pl < 10 and la < 5 else 0) for pl, la in zip(packetLoss, latency)]

    # Calculate feature importance
    classifier = ExtraTreesClassifier(random_state=50)
    start_time = time.time()
    classifier.fit(features, classificationTargets)
    importances_classification = classifier.feature_importances_
    execution_time = time.time() - start_time

    # Get indices of selected features
    selected_indices = [i for i, importance in enumerate(importances_classification) if importance > 0]
    print('Execution time in seconds:', execution_time)
    print('Selected feature indices:', selected_indices)

    return selected_indices
########################################################################################
# Analyze and select features
def analyzeFeatureSelection(selected_features):
    new_features = []
    new_targets_pl = []
    new_targets_la = []

    # Process each dataset file and apply feature selection
    for i in range(1, c + 1):
        data_path = f'/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/raw/dataset_with_all_features{i}.json'
        with open(data_path) as file:
            data = json.load(file)

            # Transform and select features
            features_transformed = [np.array(option, dtype=int)[selected_features].tolist() for option in data['features']]
            new_features.extend(features_transformed)

            # Create binary targets for packet loss and latency based on conditions
            new_targets_pl.extend(data['packetloss'])
            new_targets_la.extend( data['latency'])

    # Create binary class targets based on packet loss and latency conditions
    new_class_targets = [1 if pl < 10 and la < 5 else 0 for pl, la in zip(new_targets_pl, new_targets_la)]

    # Define the output dataset
    dataset_with_selected_features = {
        'features': new_features,
        'labels': new_class_targets
    }

    # Save the processed dataset
    output_file_path = '/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/Binary_dataset_with_selected_features.json'
    with open(output_file_path, 'w') as outfile:
        json.dump(dataset_with_selected_features, outfile, indent=4)
    print("Processed dataset saved to:", output_file_path)

    return new_features, new_class_targets

# Call the loadData function to get the selected feature indices
selected_features = loadData()

# Call analyzeFeatureSelection with the selected features
analyzeFeatureSelection(selected_features)

## plotting the Unbalancing of the whole datasets

import matplotlib.pyplot as plt
import seaborn as sns

# Define custom colors (from the uploaded image)
custom_colors = ['#D44741', '#6A56A6']

# Labels and their counts before sampling
labels = [0, 1]
counts_before = [42573, 22227]

# Plot bar chart for class distribution before sampling
plt.figure(figsize=(7, 5))
sns.barplot(x=labels, y=counts_before, palette=custom_colors * 2)
plt.title("Class Distribution Before Sampling")
plt.xlabel("Class Labels")
plt.ylabel("Number of Instances")
plt.tight_layout()
plt.show()

# === Plotting Class Distribution in Train/Test Sets and Dataset Summary ===

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# === Load the JSON dataset ===
input_path = '/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/Binary_dataset_with_selected_features.json'

with open(input_path, 'r') as f:
    data = json.load(f)

# === Extract features and labels ===
x = np.array(data['features'])
y = np.array(data['labels'])

# === Display total dataset size and class distribution ===
total_instances = len(y)
overall_counts = pd.Series(y).value_counts().sort_index()
print(f"Total instances in dataset: {total_instances}")
print("\nOverall Class Distribution:")
print(overall_counts)

# === Split into training and testing sets ===
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.20, random_state=42, stratify=y
)

# === Count class distributions ===
train_label_counts = pd.Series(y_train).value_counts().sort_index()
test_label_counts = pd.Series(y_test).value_counts().sort_index()

# === Print class distributions ===
print("\nClass Distribution in Training Data:")
print(train_label_counts)
print("\nClass Distribution in Test Data:")
print(test_label_counts)

# === Plotting ===
sns.set(style="white")  # Remove grid lines completely
fig, axs = plt.subplots(1, 2, figsize=(11, 4), sharey=True)
custom_colors = ['#1f77b4', '#ff7f0e']  # Springer-compatible high-contrast palette

# Training Data Plot
sns.barplot(
    ax=axs[0],
    x=train_label_counts.index.astype(str),
    y=train_label_counts.values,
    palette=custom_colors
)
axs[0].set_title("Training Set", fontsize=13, fontweight='bold')
axs[0].set_xlabel("Class", fontsize=11)
axs[0].set_ylabel("Instances", fontsize=11)
axs[0].grid(False)  # Disable grid lines

# Test Data Plot
sns.barplot(
    ax=axs[1],
    x=test_label_counts.index.astype(str),
    y=test_label_counts.values,
    palette=custom_colors
)
axs[1].set_title("Testing Set", fontsize=13, fontweight='bold')
axs[1].set_xlabel("Class", fontsize=11)
axs[1].set_ylabel("")  # Shared y-axis
axs[1].grid(False)  # Disable grid lines

# Final Layout and Save
plt.tight_layout()
plt.savefig("/content/drive/MyDrive/class_distribution_train_test.pdf", format='pdf', dpi=300, bbox_inches='tight')
plt.show()

"""✅ ✅ Part 1: Training and Evaluation Summary"""

# === AutoGluon Setup and Imports ===
from autogluon.tabular import TabularPredictor
from autogluon.common import space
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import (
    accuracy_score, f1_score, matthews_corrcoef,
    confusion_matrix
)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
import json

# === Mount Google Drive ===
from google.colab import drive
drive.mount('/content/drive')

# === Load data ===

with open('/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/Binary_dataset_with_selected_features.json', 'r') as f:

    data = json.load(f)

x = np.array(data['features'])
y = np.array(data['labels'])

# === Train-test split ===
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, stratify=y)

# === Normalize ===
scaler = MinMaxScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# === Prepare DataFrames ===
train_data = pd.DataFrame(x_train_scaled)
train_data['label'] = y_train
test_data = pd.DataFrame(x_test_scaled)
test_data['label'] = y_test

# === Hyperparameters and Search Setup ===
hyperparameters = {
    'NN_TORCH': {},
    'FASTAI': {},
}
time_limit = 10 * 60
num_trials = 5

hyperparameter_tune_kwargs = {
    'num_trials': num_trials,
    'scheduler': 'local',
    'searcher': 'random',
}

# === Train AutoGluon Predictor ===
start_time = time.time()
predictor = TabularPredictor(label='label').fit(
    train_data=train_data,
    time_limit=time_limit,
    hyperparameters=hyperparameters,
    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,
    verbosity=4
)
training_time_autogluon = time.time() - start_time

# === Predictions and Metrics ===
predictions = predictor.predict(test_data)

accuracy = accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions, average='binary')
mcc = matthews_corrcoef(y_test, predictions)

cm = confusion_matrix(y_test, predictions)
tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
sensitivity = tp / (tp + fn) if (tp + fn) else 0
specificity = tn / (tn + fp) if (tn + fp) else 0
gmean = np.sqrt(sensitivity * specificity)

print("\n✅ AutoGluon Evaluation Summary:")
print(f" - Training Time: {training_time_autogluon:.2f} seconds")
print(f" - Accuracy     : {accuracy:.4f}")
print(f" - F1 Score     : {f1:.4f}")
print(f" - MCC          : {mcc:.4f}")
print(f" - G-Mean       : {gmean:.4f}")

##########################################################################################################################
# Get best model name (top model in leaderboard)
best_model = predictor.model_names()[0]
print(f"\nBest Model: {best_model}")

##########################################################################################################################
# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(y_test, predictions_proba.iloc[:, 1])  # Class 1 probabilities
roc_auc = auc(fpr, tpr)

# Compute Precision-Recall curve and AP
precision, recall, _ = precision_recall_curve(y_test, predictions_proba.iloc[:, 1])
average_precision = average_precision_score(y_test, predictions_proba.iloc[:, 1])

# Plot ROC and PR Curves
plt.figure(figsize=(12, 6))

# ROC Curve
plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')

# Precision-Recall Curve
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='green', label=f'PR curve (AP = {average_precision:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')

plt.tight_layout()
plt.show()

# === Save before show ===
save_path = "/content/drive/MyDrive/class_distribution_train_test.pdf"
plt.savefig(save_path, format='pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f"Plot saved to: {save_path}")

✅ ✅ Part 2: Leaderboard, Summary

# Display top 10 models from leaderboard
leaderboard = predictor.leaderboard(test_data, extra_info=True, silent=True)
print(leaderboard.head(10))

#Loop through the top 10 models
for idx in range(min(10, len(leaderboard))):
    model_name = leaderboard.iloc[idx]['model']
    print(f"\nModel {idx+1}: {model_name}")

    # Load model object
    model = predictor._trainer.load_model(model_name)

    # Display hyperparameters
    print(f"Hyperparameters for {model_name}:\n{model.params}")

from sklearn.metrics import confusion_matrix

print("\nPerformance of Each Model in the Leaderboard:")
print("------------------------------------------------")
for model_name in predictor.model_names():
    # Predict using the specific model
    test_pred_model = predictor.predict(test_data.drop(columns=['label']), model=model_name)

    # Calculate metrics
    acc = accuracy_score(y_test, test_pred_model)
    f1 = f1_score(y_test, test_pred_model, average='binary')  # binary classification
    mcc = matthews_corrcoef(y_test, test_pred_model)

    # G-Mean calculation
    cm = confusion_matrix(y_test, test_pred_model)
    tn, fp, fn, tp = cm.ravel()
    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    gmean = (sensitivity * specificity) ** 0.5

    # Print results
    print(f"Model: {model_name}")
    print(f"  Accuracy : {acc:.4f}")
    print(f"  F1 Score : {f1:.4f}")
    print(f"  MCC      : {mcc:.4f}")
    print(f"  G-Mean   : {gmean:.4f}")
    print("------------------------------------------------")

✅ ✅ Part 3: Baseline Models

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import numpy as np
import json
import time
from google.colab import drive

drive.mount('/content/drive')

with open('/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/Binary_dataset_with_selected_features.json', 'r') as f:

    data = json.load(f)

x = np.array(data['features'])
y = np.array(data['labels'])

# Split and scale
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)
# === Normalize ===
scaler = MinMaxScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Define baseline models with default settings
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),}

# Evaluation function
def evaluate_model(name, model):
    start_train = time.time()
    model.fit(x_train, y_train)
    train_time = time.time() - start_train

    start_pred = time.time()
    y_pred = model.predict(x_test)
    pred_time = time.time() - start_pred

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    mcc = matthews_corrcoef(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    sensitivity = tp / (tp + fn) if (tp + fn) else 0
    specificity = tn / (tn + fp) if (tn + fp) else 0
    gmean = np.sqrt(sensitivity * specificity)

    print(f"\n========== {name} (Default) ==========")
    print(f"Training Time   : {train_time:.4f} seconds")
    print(f"Prediction Time : {pred_time:.4f} seconds")
    print(f"Accuracy        : {acc:.4f}")
    print(f"F1 Score        : {f1:.4f}")
    print(f"MCC             : {mcc:.4f}")
    print(f"G-Mean          : {gmean:.4f}")

# Run evaluations
for model_name, model in models.items():
    evaluate_model(model_name, model)

# =============================== #
#  AutoGluon Full Ablation Study  #
# =============================== #

from autogluon.tabular import TabularPredictor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, confusion_matrix
from math import sqrt
import pandas as pd
import numpy as np
import time
import json

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# === Load JSON data ===
with open('/content/drive/MyDrive/DELTAIoT1.1_DATASETS_used_for_JOURNAL/DELTAIoT1/Binary_dataset_with_selected_features.json', 'r') as f:
    data = json.load(f)

x = np.array(data['features'])
y = np.array(data['labels'])

# === Train-test split & scaling ===
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# === Convert to DataFrame ===
train_data = pd.DataFrame(x_train)
train_data['label'] = y_train
test_data = pd.DataFrame(x_test)
test_data['label'] = y_test

# === Define fixed model types to tune ===
hyperparameters = {
    'NN_TORCH': {},
    'FASTAI': {},
}

# === G-Mean function for binary classification ===
def geometric_mean_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    if cm.shape[0] != 2:
        return np.nan  # G-Mean only valid for binary
    tn, fp, fn, tp = cm.ravel()
    sensitivity = tp / (tp + fn) if (tp + fn) else 0
    specificity = tn / (tn + fp) if (tn + fp) else 0
    return sqrt(sensitivity * specificity)

# === Ablation configs (includes 'scheduler' to avoid KeyError) ===
ablation_configs = [
    {'name': 'TL600', 'num_trials': 5, 'searcher': 'random', 'scheduler': 'local', 'time_limit': 600,},
    {'name': 'TL900', 'num_trials': 5, 'searcher': 'random', 'scheduler': 'local', 'time_limit': 900,},

    {'name': 'NT5', 'time_limit': 300, 'searcher': 'random', 'scheduler': 'local', 'num_trials': 5},
    {'name': 'NT10', 'time_limit': 300, 'searcher': 'random', 'scheduler': 'local', 'num_trials': 10},

    {'name': 'SRCH_random', 'time_limit': 300, 'num_trials': 5, 'scheduler': 'local', 'searcher': 'random'},
    {'name': 'SRCH_bayesopt', 'time_limit': 300, 'num_trials': 5, 'scheduler': 'local', 'searcher': 'bayes'},
]

# === Run ablation experiments ===
results = []

for config in ablation_configs:
    print(f"\n===== Running: {config['name']} =====")

    hpo_kwargs = {
        'num_trials': config['num_trials'],
        'searcher': config['searcher'],
        'scheduler': config['scheduler']
    }

    start_time = time.time()
    try:
        predictor = TabularPredictor(label='label').fit(
            train_data=train_data,
            time_limit=config['time_limit'],
            hyperparameters=hyperparameters,
            hyperparameter_tune_kwargs=hpo_kwargs
        )
        train_time = time.time() - start_time

        leaderboard = predictor.leaderboard(test_data, silent=True)
        print("\nLeaderboard:")
        print(leaderboard)

        test_pred = predictor.predict(test_data.drop(columns=['label']))
        acc = accuracy_score(y_test, test_pred)
        f1 = f1_score(y_test, test_pred)
        mcc = matthews_corrcoef(y_test, test_pred)
        gmean = geometric_mean_score(y_test, test_pred)

        best_model = leaderboard.iloc[0]['model']

        results.append({
            'Experiment': config['name'],
            'Accuracy': acc,
            'F1 Score': f1,
            'MCC': mcc,
            'G-Mean': gmean,
            'Train Time (s)': train_time,
            'Best Model': best_model,
        })

        print(f"Best model: {best_model} | Accuracy: {acc:.4f} | F1: {f1:.4f} | MCC: {mcc:.4f} | G-Mean: {gmean:.4f} | Time: {train_time:.2f}s")

    except Exception as e:
        print(f"Experiment {config['name']} failed: {e}")
        results.append({
            'Experiment': config['name'],
            'Accuracy': np.nan,
            'F1 Score': np.nan,
            'MCC': np.nan,
            'G-Mean': np.nan,
            'Train Time (s)': np.nan,
            'Best Model': None,
        })

# === Summary Results ===
results_df = pd.DataFrame(results)
print("\n===== Final Ablation Results =====")
print(results_df)

# === Save results to Drive in both CSV and Excel formats for readability ===
save_path_xlsx = '/content/drive/MyDrive/autogluon_full_ablation_results.xlsx'

# Save as CSV
results_df.to_csv(save_path_csv, index=False)

# Save as Excel (easier to read for humans)
results_df.to_excel(save_path_xlsx, index=False)

print(f"\nResults saved successfully to:\nCSV: {save_path_csv}\nExcel: {save_path_xlsx}")

